{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIB - Portfolio of Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12 - Dropout layer\n",
    "\n",
    "A dropout layer in neural networks (NNs) is a regularization technique where a random set of neurons is temporarily ignored (dropped out) during training. This helps prevent overfitting by promoting robustness and generalization in the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1) \n",
    "Add a new layer named Dropout to the layers module; Take into consideration the following structure:\n",
    "\n",
    "class Dropout(Layer):\n",
    "- arguments:\n",
    "  - probability – the dropout rate, between 0 and 1\n",
    "- estimated parameters:\n",
    "  - mask – binomial mask that sets some inputs to 0 based on the probability\n",
    "  - input – the input of the layer\n",
    "  - output – the output of the layer\n",
    "- methods:\n",
    "  - forward_propagation – performs forward propagation on the given input, i.e., applies the mask to the input and scales it when in training mode; does nothing in inference mode (returns the input as is)\n",
    "  - backward_propagation – performs backward propagation on the given error, i.e., multiplies the received error by the mask\n",
    "  - output_shape – returns the input shape (dropout does not change the shape of the data)\n",
    "  - parameters – returns 0 (dropout layers do not have learnable parameters)\n",
    "\n",
    "---\n",
    "\n",
    "Dropout.forward_propagation:\n",
    "- arguments:\n",
    "  - input – input array\n",
    "  - training – boolean indicating whether we are in training or inference mode\n",
    "- algorithm:\n",
    "  1. If we are in training mode:\n",
    "     - Compute the scaling factor: `scaling_factor = 1 / (1 – probability)`\n",
    "     - Compute the mask using a binomial distribution with probability `1 – probability` and size equal to the input\n",
    "     - Compute the output: `output = input * mask * scaling_factor`\n",
    "     - Return the output\n",
    "  2. If we are in inference mode:\n",
    "     - Return the input (input is not changed during inference)\n",
    "\n",
    "---\n",
    "\n",
    "Dropout.backward_propagation:\n",
    "- arguments:\n",
    "  - output_error – the output error of the layer\n",
    "- algorithm:\n",
    "  - Multiply the `output_error` by the `mask` and return it\n",
    "\n",
    "---\n",
    "\n",
    "Dropout.output_shape:\n",
    "- Returns the input shape\n",
    "\n",
    "---\n",
    "\n",
    "Dropout.parameters:\n",
    "- Returns 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2) \n",
    "Test the layer with a random input and check if the output shows the desired behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.neural_networks.layers import Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Input:\n",
      "[[ 0.59409987  2.99990412  2.79225544]\n",
      " [-1.08594183 -0.28161022  0.67585272]\n",
      " [ 0.72023149  0.52827755  1.63405624]]\n",
      "Output with Dropout:\n",
      "[[ 0.          5.99980823  0.        ]\n",
      " [-2.17188366 -0.          1.35170544]\n",
      " [ 0.          1.05655509  0.        ]]\n",
      "Applied Mask:\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "\n",
      "Inference:\n",
      "Input:\n",
      "[[ 0.59409987  2.99990412  2.79225544]\n",
      " [-1.08594183 -0.28161022  0.67585272]\n",
      " [ 0.72023149  0.52827755  1.63405624]]\n",
      "Output with Dropout (no change):\n",
      "[[ 0.59409987  2.99990412  2.79225544]\n",
      " [-1.08594183 -0.28161022  0.67585272]\n",
      " [ 0.72023149  0.52827755  1.63405624]]\n"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.layers import Dropout\n",
    "import numpy as np\n",
    "probability = 0.5  # Dropout rate (50%)\n",
    "input_data = np.random.randn(3, 3)  # Random input (3x3)\n",
    "\n",
    "# Create Dropout layer\n",
    "dropout_layer = Dropout(probability=probability)\n",
    "\n",
    "# Test during training\n",
    "print(\"Training:\")\n",
    "output_train = dropout_layer.forward_propagation(input_data, training=True)\n",
    "print(\"Input:\")\n",
    "print(input_data)\n",
    "print(\"Output with Dropout:\")\n",
    "print(output_train)\n",
    "print(\"Applied Mask:\")\n",
    "print(dropout_layer.mask)\n",
    "\n",
    "# Test during inference (without dropout)\n",
    "print(\"\\nInference:\")\n",
    "output_infer = dropout_layer.forward_propagation(input_data, training=False)\n",
    "print(\"Input:\")\n",
    "print(input_data)\n",
    "print(\"Output with Dropout (no change):\")\n",
    "print(output_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos ver que após a aplicação da máscara, durante o treino a saída, passe a ter neurónios desativados (0) e ativados, enquanto que no processo de inferência isso não acontece... Os tamanhos das entradas e saídas são iguais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
